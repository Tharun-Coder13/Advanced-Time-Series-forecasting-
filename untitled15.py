# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1arqpu-STWsEa30r7D633WsbKGMRqVAVN
"""

#######################################################################
#  Advanced Time Series Forecasting - Single Complete Python Script
#######################################################################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from datetime import datetime, timedelta

np.random.seed(42)
tf.random.set_seed(42)

# -------------------------------------------------------------------
# 1. Generate Synthetic Multivariate Time Series (5 features, 3 years)
# -------------------------------------------------------------------

def generate_dataset(days=3*365, n_features=5):
    start = datetime(2019,1,1)
    dates = [start + timedelta(days=i) for i in range(days)]
    t = np.arange(days)

    trend = 0.001 * t
    yearly = 2*np.sin(2*np.pi*t/365)
    weekly = 0.7*np.sin(2*np.pi*t/7)

    data = []
    base = 5 + trend + yearly + weekly + np.random.normal(0, 0.3, days)
    data.append(base)

    for i in range(1, n_features):
        feat = 3 + trend*(0.5+i*0.2) + yearly*(0.6+i*0.1) + weekly*(1-i*0.05)
        feat = feat + 0.3 * np.roll(base, i) + np.random.normal(0, 0.4, days)
        data.append(feat)

    df = pd.DataFrame(np.array(data).T, columns=[f"feat_{i}" for i in range(n_features)])
    df.index = dates
    return df

# -------------------------------------------------------------------
# 2. Create lookback windows for forecasting
# -------------------------------------------------------------------

def create_windows(X, lookback=60, horizon=7):
    enc, dec = [], []
    T = X.shape[0]

    for i in range(T - lookback - horizon):
        enc.append(X[i:i+lookback])
        dec.append(X[i+lookback:i+lookback+horizon, 0:1])
    return np.array(enc), np.array(dec)

# -------------------------------------------------------------------
# 3. Transformer Seq2Seq Model
# -------------------------------------------------------------------

def build_transformer(input_shape, lb, horizon, d_model=64, heads=4, dff=128):

    enc_in = Input(shape=input_shape)
    dec_in = Input(shape=(horizon,1))

    enc_x = layers.Dense(d_model)(enc_in)
    pos = layers.Embedding(lb, d_model)(tf.range(lb))
    enc_x = enc_x + pos

    for _ in range(2):
        attn = layers.MultiHeadAttention(num_heads=heads, key_dim=d_model//heads)
        x1 = attn(enc_x, enc_x)
        x1 = layers.LayerNormalization()(x1 + enc_x)
        f = layers.Dense(dff, activation='relu')(x1)
        f = layers.Dense(d_model)(f)
        enc_x = layers.LayerNormalization()(x1 + f)

    dec_x = layers.Dense(d_model)(dec_in)
    dpos = layers.Embedding(horizon, d_model)(tf.range(horizon))
    dec_x = dec_x + dpos

    for _ in range(2):
        self_attn = layers.MultiHeadAttention(num_heads=heads, key_dim=d_model//heads)
        s = self_attn(dec_x, dec_x)
        s = layers.LayerNormalization()(s + dec_x)

        cross = layers.MultiHeadAttention(num_heads=heads, key_dim=d_model//heads)
        c = cross(s, enc_x)
        c = layers.LayerNormalization()(c + s)

        ff = layers.Dense(dff, activation='relu')(c)
        ff = layers.Dense(d_model)(ff)
        dec_x = layers.LayerNormalization()(c + ff)

    out = layers.Dense(1)(dec_x)

    model = Model([enc_in, dec_in], out)
    model.compile(optimizer='adam', loss='mse',
                  metrics=[tf.keras.metrics.RootMeanSquaredError(),
                           tf.keras.metrics.MeanAbsoluteError()])
    return model

# -------------------------------------------------------------------
# 4. LSTM Baseline Model
# -------------------------------------------------------------------

def build_lstm(input_shape, horizon):
    inp = Input(shape=input_shape)
    x = layers.LSTM(128, return_sequences=False)(inp)
    x = layers.RepeatVector(horizon)(x)
    x = layers.LSTM(128, return_sequences=True)(x)
    out = layers.TimeDistributed(layers.Dense(1))(x)

    model = Model(inp, out)
    model.compile(optimizer='adam', loss='mse',
                  metrics=[tf.keras.metrics.RootMeanSquaredError(),
                           tf.keras.metrics.MeanAbsoluteError()])
    return model

# -------------------------------------------------------------------
# MAIN EXECUTION
# -------------------------------------------------------------------

print("Generating dataset...")
df = generate_dataset()
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df.values)

lookback = 60
horizon = 7

enc, dec = create_windows(scaled, lookback, horizon)
print("Windows:", enc.shape, dec.shape)

# Train/Val/Test split
N = len(enc)
train_end = int(N*0.7)
val_end = int(N*0.85)

enc_train, dec_train = enc[:train_end], dec[:train_end]
enc_val, dec_val = enc[train_end:val_end], dec[train_end:val_end]
enc_test, dec_test = enc[val_end:], dec[val_end:]

# Teacher-forcing decoder inputs
dec_in_train = np.zeros_like(dec_train)
dec_in_train[:,1:,:] = dec_train[:,:-1,:]

dec_in_val = np.zeros_like(dec_val)
dec_in_val[:,1:,:] = dec_val[:,:-1,:]

dec_in_test = np.zeros_like(dec_test)
dec_in_test[:,1:,:] = dec_test[:,:-1,:]

# -----------------------------
# Train Transformer
# -----------------------------
print("\nTraining Transformer...")
transformer = build_transformer((lookback, df.shape[1]), lookback, horizon)
transformer.summary()

transformer.fit([enc_train, dec_in_train], dec_train,
                validation_data=([enc_val, dec_in_val], dec_val),
                epochs=10, batch_size=64, verbose=2)

# -----------------------------
# Train LSTM
# -----------------------------
print("\nTraining LSTM...")
lstm = build_lstm((lookback, df.shape[1]), horizon)
lstm.summary()

lstm.fit(enc_train, dec_train,
         validation_data=(enc_val, dec_val),
         epochs=10, batch_size=64, verbose=2)

# -----------------------------
# Evaluate
# -----------------------------
print("\nEvaluating...")

t_pred = transformer.predict([enc_test, dec_in_test])
l_pred = lstm.predict(enc_test)

# inverse transform for feature 0 only
def inverse(pred, enc_ref):
    N,H,_ = pred.shape
    out = np.zeros((N,H))
    for i in range(N):
        for h in range(H):
            temp = np.zeros(df.shape[1])
            temp[0] = pred[i,h,0]
            temp[1:] = enc_ref[i,-1,1:]
            out[i,h] = scaler.inverse_transform(temp.reshape(1,-1))[0,0]
    return out

t_inv = inverse(t_pred, enc_test)
l_inv = inverse(l_pred, enc_test)
true_inv = inverse(dec_test, enc_test)

# metrics
def metrics(y, yhat):
    return mean_squared_error(y.flatten(), yhat.flatten(), squared=False), \
           mean_absolute_error(y.flatten(), yhat.flatten())

t_rmse, t_mae = metrics(true_inv, t_inv)
l_rmse, l_mae = metrics(true_inv, l_inv)

print(f"\nTransformer  RMSE={t_rmse:.4f}  MAE={t_mae:.4f}")
print(f"LSTM         RMSE={l_rmse:.4f}  MAE={l_mae:.4f}")

# -----------------------------
# Plotting h=1 predictions
# -----------------------------
plt.figure(figsize=(10,5))
plt.plot(true_inv[-200:,0], label="True")
plt.plot(t_inv[-200:,0], label="Transformer")
plt.plot(l_inv[-200:,0], label="LSTM")
plt.legend()
plt.title("Forecast Comparison (h=1)")
plt.savefig("forecast_plot.png")
print("\nSaved forecast_plot.png")

print("\nDONE.")